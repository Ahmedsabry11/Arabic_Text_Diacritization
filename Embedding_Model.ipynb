{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qd5IuOVjtji3"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install gensim\n",
        "!pip install nltk\n",
        "!pip install tokenizers\n",
        "!pip install sentencepiece\n",
        "!pip install python-bidi\n",
        "!pip install arabic-reshaper\n",
        "# !pip install PyArabic\n",
        "!pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVof0I1Bvhfm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import pandas as pd\n",
        "# from pyarabic.araby import tokenize, strip_tashkeel\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from utilities import *\n",
        "import textProcessing as tp\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import word2vec\n",
        "import spacy\n",
        "# nltk.download('punkt')\n",
        "# train_text = load_text(\"dataset/train.txt\")\n",
        "# tp.preprocessing_text(train_text,\"train_preprocessed.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilaf9_Jmxsg4",
        "outputId": "af18fa5a-65ca-4e18-e441-498d3c19fd05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-12-28 19:54:48--  https://archive.org/download/aravec2.0/tweet_cbow_300.zip\n",
            "Resolving archive.org (archive.org)... 207.241.224.2\n",
            "Connecting to archive.org (archive.org)|207.241.224.2|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ia803107.us.archive.org/0/items/aravec2.0/tweet_cbow_300.zip [following]\n",
            "--2023-12-28 19:54:48--  https://ia803107.us.archive.org/0/items/aravec2.0/tweet_cbow_300.zip\n",
            "Resolving ia803107.us.archive.org (ia803107.us.archive.org)... 207.241.232.157\n",
            "Connecting to ia803107.us.archive.org (ia803107.us.archive.org)|207.241.232.157|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 746111232 (712M) [application/zip]\n",
            "Saving to: ‘tweet_cbow_300.zip’\n",
            "\n",
            "tweet_cbow_300.zip  100%[===================>] 711.55M   943KB/s    in 15m 22s \n",
            "\n",
            "2023-12-28 20:10:10 (791 KB/s) - ‘tweet_cbow_300.zip’ saved [746111232/746111232]\n",
            "\n",
            "Archive:  tweet_cbow_300.zip\n",
            "  inflating: tweets_cbow_300         \n",
            "  inflating: tweets_cbow_300.trainables.syn1neg.npy  \n",
            "  inflating: tweets_cbow_300.wv.vectors.npy  \n"
          ]
        }
      ],
      "source": [
        "# Download via terminal commands\n",
        "# !wget \"https://archive.org/download/aravec2.0/tweet_cbow_300.zip\"\n",
        "# !unzip \"tweet_cbow_300.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qG_9OjV9SN7"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48wnuE9TICHz",
        "outputId": "e8dc02b6-de30-4842-a3fe-a342b9057e19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-12-28 20:13:42.548291: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-28 20:13:42.548343: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-28 20:13:42.549584: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-28 20:13:43.637177: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\u001b[38;5;4mℹ Creating blank nlp object for language 'ar'\u001b[0m\n",
            "331679it [00:33, 9867.64it/s] \n",
            "\u001b[38;5;2m✔ Successfully converted 331679 vectors\u001b[0m\n",
            "\u001b[38;5;2m✔ Saved nlp object with vectors to output directory. You can now use\n",
            "the path to it in your config as the 'vectors' setting in [initialize].\u001b[0m\n",
            "/content/spacy.aravec.model\n"
          ]
        }
      ],
      "source": [
        "# # make a directory called \"spacyModel\"\n",
        "# %mkdir spacyModel\n",
        "# model = gensim.models.Word2Vec.load(\"tweets_cbow_300\")\n",
        "# # export the word2vec fomart to the directory\n",
        "# model.wv.save_word2vec_format(\"./spacyModel/aravec.txt\")\n",
        "# # using `gzip` to compress the .txt file\n",
        "# !gzip ./spacyModel/aravec.txt\n",
        "!python -m spacy init vectors ar ./spacyModel/aravec.txt.gz spacy.aravec.model\n",
        "nlp = spacy.load(\"./spacy.aravec.model/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "6a6o7z8GsZnS"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import word2vec\n",
        "import spacy\n",
        "from torch import mode\n",
        "\n",
        "class AraVecEmbbedding():\n",
        "    def __init__(self,model):\n",
        "        self.model=model\n",
        "\n",
        "    def map_words_to_vectors(self,sentence):\n",
        "        return [(token.text,token.vector) for token in self.model(sentence)]\n",
        "\n",
        "\n",
        "    def AraVec_wordEmbedding(self,sentence):\n",
        "        return self.map_words_to_vectors(sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6nUKjuy7shiG"
      },
      "outputs": [],
      "source": [
        "from utilities import *\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import textProcessing as tp\n",
        "import torch\n",
        "import numpy as np\n",
        "from data_preprocessing import DataPreprocessing\n",
        "import re\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self,model,dataset_path=\"dataset/train_preprocessed.txt\",T=100):\n",
        "        self.dataPreprocessor = DataPreprocessing()\n",
        "        self.T = T\n",
        "        self.data = load_text(dataset_path)\n",
        "        self.wordEmbedding=AraVecEmbbedding(model)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        sentence = self.data[idx]\n",
        "        # extract the label\n",
        "        labels,sentence= self.dataPreprocessor.extract_diacritics_with_previous_letter('s'+sentence)\n",
        "\n",
        "        # check sentence length\n",
        "        if len(sentence) > self.T:\n",
        "            sentence = sentence[:self.T]\n",
        "            labels = labels[:self.T]\n",
        "        else:\n",
        "            for i in range(self.T - len(sentence)):\n",
        "                sentence += '0'\n",
        "                labels.append('0')\n",
        "\n",
        "        assert len(sentence) == self.T\n",
        "        assert len(labels) == self.T\n",
        "\n",
        "        # convert the sentence to embedding\n",
        "        # Word\n",
        "        arabic_words_with_spaces =  re.findall(r'\\s*\\b\\w+\\b\\s*|,\\s*|\\s*\\b\\w+\\b(?!\\s*$)',sentence)\n",
        "        sentence_char = self.dataPreprocessor.convert_sentence_to_vector(sentence)\n",
        "        sentence_word_embedding =self.wordEmbedding.AraVec_wordEmbedding(sentence)\n",
        "        list_of_char = []\n",
        "        for i, (word, word_embedding) in enumerate(sentence_word_embedding):\n",
        "          length_arabic = len(arabic_words_with_spaces[i])\n",
        "          for j in range(length_arabic):\n",
        "            li = sentence_char[j].astype(float)\n",
        "            concatenated_array = np.concatenate((li, np.array(word_embedding)), axis=0)\n",
        "            list_of_char.append(concatenated_array)\n",
        "        assert len(list_of_char) == self.T\n",
        "        assert len(labels) == self.T\n",
        "        # convert the labels to one hot encoding\n",
        "        labels = self.dataPreprocessor.convert_labels_to_indices(labels)\n",
        "\n",
        "        # reshape the labels\n",
        "        labels = labels.reshape(-1,1)\n",
        "\n",
        "        # convert the sentence and labels to tensors\n",
        "        sentence = torch.tensor(list_of_char, dtype=torch.float32)\n",
        "        labels = torch.LongTensor(labels)\n",
        "\n",
        "        return sentence, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "LA2yfGo0Wjem"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from LSTM import LSTMClassifier\n",
        "from data_preprocessing import DataPreprocessing\n",
        "\n",
        "class LSTMTrainer:\n",
        "    def __init__(self,load=True,epoch = 0,input_size = 39,hidden_size = 128,output_size = 16,batch_size = 512,num_epochs = 20):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.model = LSTMClassifier(self.input_size, self.hidden_size, self.output_size)\n",
        "        self.current_epoch = 0\n",
        "        self.current_epoch = epoch\n",
        "        if load:\n",
        "            self.load_model(epoch)\n",
        "        self.model.to(self.device)\n",
        "        self.dataset = MyDataset(model = nlp,T = 280)\n",
        "        self.test_dataset = MyDataset(model = nlp,dataset_path=\"dataset/test_preprocessed.txt\",T = 280)\n",
        "        self.train_dataloader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        self.test_dataloader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=15)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.0001)\n",
        "        self.scheduler = StepLR(self.optimizer, step_size=2, gamma=0.5)\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(0,self.current_epoch):\n",
        "            self.scheduler.step()\n",
        "        for epoch in range(self.current_epoch,self.num_epochs):\n",
        "            self.model.train()\n",
        "            running_loss = 0.0\n",
        "            for i, data in enumerate(self.train_dataloader, 0):\n",
        "                # get the inputs\n",
        "                inputs, labels = data\n",
        "                # print(\"input shape\",inputs.size())\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "                # zero the parameter gradients\n",
        "                self.optimizer.zero_grad()\n",
        "                # forward + backward + optimize\n",
        "                outputs = self.model(inputs)\n",
        "                outputs = outputs.view(-1, outputs.shape[-1])\n",
        "                labels = labels.view(-1)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
        "                self.optimizer.step()\n",
        "                # print statistics\n",
        "                running_loss += loss.item()\n",
        "                if i % 2 == 0:  # print every 100 mini-batches\n",
        "                    print('[%d, %5d] loss: %.3f' %\n",
        "                          (epoch + 1, i + 1, running_loss / 2))\n",
        "                    running_loss = 0.0\n",
        "            self.scheduler.step()\n",
        "            # save the model if it has the best training loss till now\n",
        "            self.save_model(epoch+1)\n",
        "        print('Finished Training')\n",
        "\n",
        "    def test(self):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            running_loss = 0.0\n",
        "            for i, data in enumerate(self.test_dataloader, 0):\n",
        "                # get the inputs\n",
        "                inputs, labels = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "                # forward + backward + optimize\n",
        "                outputs = self.model(inputs)\n",
        "                outputs = outputs.view(-1, outputs.shape[-1])\n",
        "                labels = labels.view(-1)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                running_loss += loss.item()\n",
        "            print('Test loss: %.3f' %\n",
        "                  (running_loss / len(self.test_dataloader)))\n",
        "    def calcluate_accuracy(self):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for i, data in enumerate(self.test_dataloader, 0):\n",
        "                # get the inputs\n",
        "                inputs, labels = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "                # forward + backward + optimize\n",
        "                outputs = self.model(inputs)\n",
        "                outputs = outputs.view(-1, outputs.shape[-1])\n",
        "                labels = labels.view(-1)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                      # cut the padding\n",
        "                predicted = predicted[labels != 15]\n",
        "                labels = labels[labels != 15]\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            print('Accuracy of the network on the test set: %d %%' % (\n",
        "                    100 * correct / total))\n",
        "            print('Accuracy of the network on the test set: %f %%' % (\n",
        "                    100 * correct / total))\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for i, data in enumerate(self.train_dataloader, 0):\n",
        "                # get the inputs\n",
        "                inputs, labels = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "                # forward + backward + optimize\n",
        "                outputs = self.model(inputs)\n",
        "                outputs = outputs.view(-1, outputs.shape[-1])\n",
        "                labels = labels.view(-1)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                      # cut the padding\n",
        "                predicted = predicted[labels != 15]\n",
        "                labels = labels[labels != 15]\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            print('Accuracy of the network on the train set: %d %%' % (\n",
        "                    100 * correct / total))\n",
        "            # print floating point accuracy\n",
        "            print('Accuracy of the network on the train set: %f %%' % (\n",
        "                    100 * correct / total))\n",
        "    def save_model(self,epoch):\n",
        "        torch.save(self.model.state_dict(), \"models/lstm_model_\"+str(epoch)+\".pth\")\n",
        "    def load_model(self,epoch=9):\n",
        "        self.model.load_state_dict(torch.load(\"models/lstm_model_\"+str(epoch)+\".pth\"))\n",
        "        self.model.eval()\n",
        "        self.current_epoch = epoch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "34N76AryvKcd"
      },
      "outputs": [],
      "source": [
        "# from LSTM_Training import LSTMTrainer\n",
        "lstmTrainer = LSTMTrainer(load = False,epoch = 0,input_size = 339,hidden_size = 128,output_size = 16,batch_size = 128,num_epochs = 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LccF7Ac0UC5",
        "outputId": "fac47163-5ea7-44e1-f730-50ee075136e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4,   253] loss: 0.802\n",
            "[4,   255] loss: 0.822\n",
            "[4,   257] loss: 0.805\n",
            "[4,   259] loss: 0.834\n",
            "[4,   261] loss: 0.817\n",
            "[4,   263] loss: 0.802\n",
            "[4,   265] loss: 0.832\n",
            "[4,   267] loss: 0.807\n",
            "[4,   269] loss: 0.810\n",
            "[4,   271] loss: 0.824\n",
            "[4,   273] loss: 0.804\n",
            "[4,   275] loss: 0.821\n",
            "[4,   277] loss: 0.825\n",
            "[4,   279] loss: 0.803\n",
            "[4,   281] loss: 0.813\n",
            "[4,   283] loss: 0.811\n",
            "[4,   285] loss: 0.819\n",
            "[4,   287] loss: 0.821\n",
            "[4,   289] loss: 0.810\n",
            "[4,   291] loss: 0.820\n",
            "[4,   293] loss: 0.806\n",
            "[4,   295] loss: 0.801\n",
            "[4,   297] loss: 0.815\n",
            "[4,   299] loss: 0.823\n",
            "[4,   301] loss: 0.825\n",
            "[4,   303] loss: 0.818\n",
            "[4,   305] loss: 0.804\n",
            "[4,   307] loss: 0.796\n",
            "[4,   309] loss: 0.817\n",
            "[4,   311] loss: 0.822\n",
            "[4,   313] loss: 0.802\n",
            "[4,   315] loss: 0.798\n",
            "[4,   317] loss: 0.802\n",
            "[4,   319] loss: 0.807\n",
            "[4,   321] loss: 0.795\n",
            "[4,   323] loss: 0.800\n",
            "[4,   325] loss: 0.809\n",
            "[4,   327] loss: 0.835\n",
            "[4,   329] loss: 0.808\n",
            "[4,   331] loss: 0.834\n",
            "[4,   333] loss: 0.800\n",
            "[4,   335] loss: 0.826\n",
            "[4,   337] loss: 0.820\n",
            "[4,   339] loss: 0.807\n",
            "[4,   341] loss: 0.802\n",
            "[4,   343] loss: 0.825\n",
            "[4,   345] loss: 0.816\n",
            "[4,   347] loss: 0.824\n",
            "[4,   349] loss: 0.804\n",
            "[4,   351] loss: 0.813\n",
            "[4,   353] loss: 0.818\n",
            "[4,   355] loss: 0.827\n",
            "[4,   357] loss: 0.818\n",
            "[4,   359] loss: 0.797\n",
            "[4,   361] loss: 0.795\n",
            "[4,   363] loss: 0.790\n",
            "[4,   365] loss: 0.814\n",
            "[4,   367] loss: 0.812\n",
            "[4,   369] loss: 0.793\n",
            "[4,   371] loss: 0.816\n",
            "[4,   373] loss: 0.818\n",
            "[4,   375] loss: 0.793\n",
            "[4,   377] loss: 0.825\n",
            "[4,   379] loss: 0.817\n",
            "[4,   381] loss: 0.813\n",
            "[4,   383] loss: 0.806\n",
            "[4,   385] loss: 0.783\n",
            "[4,   387] loss: 0.809\n",
            "[4,   389] loss: 0.790\n",
            "[4,   391] loss: 0.797\n",
            "[4,   393] loss: 0.805\n",
            "[4,   395] loss: 0.796\n",
            "[4,   397] loss: 0.802\n",
            "[4,   399] loss: 0.784\n",
            "[4,   401] loss: 0.805\n",
            "[4,   403] loss: 0.797\n",
            "[4,   405] loss: 0.803\n",
            "[4,   407] loss: 0.813\n",
            "[4,   409] loss: 0.804\n",
            "[4,   411] loss: 0.810\n",
            "[4,   413] loss: 0.804\n",
            "[4,   415] loss: 0.800\n",
            "[4,   417] loss: 0.799\n",
            "[4,   419] loss: 0.787\n",
            "[4,   421] loss: 0.799\n",
            "[4,   423] loss: 0.816\n",
            "[4,   425] loss: 0.821\n",
            "[4,   427] loss: 0.786\n",
            "[4,   429] loss: 0.806\n",
            "[4,   431] loss: 0.809\n",
            "[4,   433] loss: 0.794\n",
            "[4,   435] loss: 0.785\n",
            "[4,   437] loss: 0.804\n",
            "[4,   439] loss: 0.776\n",
            "[4,   441] loss: 0.807\n",
            "[4,   443] loss: 0.813\n",
            "[4,   445] loss: 0.782\n",
            "[4,   447] loss: 0.793\n",
            "[4,   449] loss: 0.798\n",
            "[4,   451] loss: 0.797\n",
            "[4,   453] loss: 0.804\n",
            "[4,   455] loss: 0.797\n",
            "[4,   457] loss: 0.798\n",
            "[4,   459] loss: 0.808\n",
            "[4,   461] loss: 0.799\n",
            "[4,   463] loss: 0.799\n",
            "[4,   465] loss: 0.835\n",
            "[4,   467] loss: 0.807\n",
            "[4,   469] loss: 0.789\n",
            "[4,   471] loss: 0.818\n",
            "[4,   473] loss: 0.799\n",
            "[4,   475] loss: 0.809\n",
            "[4,   477] loss: 0.828\n",
            "[4,   479] loss: 0.797\n",
            "[4,   481] loss: 0.799\n",
            "[4,   483] loss: 0.811\n",
            "[4,   485] loss: 0.812\n",
            "[4,   487] loss: 0.792\n",
            "[4,   489] loss: 0.806\n",
            "[4,   491] loss: 0.790\n",
            "[4,   493] loss: 0.805\n",
            "[4,   495] loss: 0.796\n",
            "[4,   497] loss: 0.812\n",
            "[4,   499] loss: 0.785\n",
            "[4,   501] loss: 0.778\n",
            "[4,   503] loss: 0.796\n",
            "[4,   505] loss: 0.788\n",
            "[4,   507] loss: 0.790\n",
            "[4,   509] loss: 0.802\n",
            "[4,   511] loss: 0.795\n",
            "[4,   513] loss: 0.805\n",
            "[4,   515] loss: 0.795\n",
            "[4,   517] loss: 0.801\n",
            "[4,   519] loss: 0.796\n",
            "[4,   521] loss: 0.812\n",
            "[4,   523] loss: 0.768\n",
            "[4,   525] loss: 0.790\n",
            "[4,   527] loss: 0.798\n",
            "[4,   529] loss: 0.784\n",
            "[4,   531] loss: 0.793\n",
            "[4,   533] loss: 0.817\n",
            "[4,   535] loss: 0.807\n",
            "[4,   537] loss: 0.796\n",
            "[4,   539] loss: 0.804\n",
            "[4,   541] loss: 0.788\n",
            "[4,   543] loss: 0.784\n",
            "[4,   545] loss: 0.802\n",
            "[4,   547] loss: 0.784\n",
            "[4,   549] loss: 0.791\n",
            "[4,   551] loss: 0.775\n",
            "[4,   553] loss: 0.804\n",
            "[4,   555] loss: 0.812\n",
            "[4,   557] loss: 0.790\n",
            "[4,   559] loss: 0.771\n",
            "[4,   561] loss: 0.775\n",
            "[4,   563] loss: 0.785\n",
            "[4,   565] loss: 0.799\n",
            "[4,   567] loss: 0.785\n",
            "[4,   569] loss: 0.788\n",
            "[4,   571] loss: 0.793\n",
            "[4,   573] loss: 0.812\n",
            "[4,   575] loss: 0.795\n",
            "[4,   577] loss: 0.781\n",
            "[4,   579] loss: 0.784\n",
            "[4,   581] loss: 0.796\n",
            "[5,     1] loss: 0.397\n",
            "[5,     3] loss: 0.779\n",
            "[5,     5] loss: 0.783\n",
            "[5,     7] loss: 0.809\n",
            "[5,     9] loss: 0.799\n",
            "[5,    11] loss: 0.803\n",
            "[5,    13] loss: 0.788\n",
            "[5,    15] loss: 0.774\n",
            "[5,    17] loss: 0.785\n",
            "[5,    19] loss: 0.781\n",
            "[5,    21] loss: 0.788\n",
            "[5,    23] loss: 0.799\n",
            "[5,    25] loss: 0.801\n",
            "[5,    27] loss: 0.810\n",
            "[5,    29] loss: 0.786\n",
            "[5,    31] loss: 0.780\n",
            "[5,    33] loss: 0.824\n",
            "[5,    35] loss: 0.793\n",
            "[5,    37] loss: 0.793\n",
            "[5,    39] loss: 0.793\n",
            "[5,    41] loss: 0.760\n",
            "[5,    43] loss: 0.783\n",
            "[5,    45] loss: 0.791\n",
            "[5,    47] loss: 0.794\n",
            "[5,    49] loss: 0.794\n",
            "[5,    51] loss: 0.806\n",
            "[5,    53] loss: 0.782\n",
            "[5,    55] loss: 0.783\n",
            "[5,    57] loss: 0.822\n",
            "[5,    59] loss: 0.795\n",
            "[5,    61] loss: 0.807\n",
            "[5,    63] loss: 0.805\n",
            "[5,    65] loss: 0.778\n",
            "[5,    67] loss: 0.826\n",
            "[5,    69] loss: 0.790\n",
            "[5,    71] loss: 0.797\n",
            "[5,    73] loss: 0.771\n",
            "[5,    75] loss: 0.790\n",
            "[5,    77] loss: 0.769\n",
            "[5,    79] loss: 0.789\n",
            "[5,    81] loss: 0.771\n",
            "[5,    83] loss: 0.782\n",
            "[5,    85] loss: 0.817\n",
            "[5,    87] loss: 0.774\n",
            "[5,    89] loss: 0.795\n",
            "[5,    91] loss: 0.810\n",
            "[5,    93] loss: 0.767\n",
            "[5,    95] loss: 0.824\n",
            "[5,    97] loss: 0.785\n",
            "[5,    99] loss: 0.784\n",
            "[5,   101] loss: 0.756\n",
            "[5,   103] loss: 0.798\n",
            "[5,   105] loss: 0.770\n",
            "[5,   107] loss: 0.796\n",
            "[5,   109] loss: 0.797\n",
            "[5,   111] loss: 0.804\n",
            "[5,   113] loss: 0.806\n",
            "[5,   115] loss: 0.791\n",
            "[5,   117] loss: 0.771\n",
            "[5,   119] loss: 0.801\n",
            "[5,   121] loss: 0.807\n",
            "[5,   123] loss: 0.792\n",
            "[5,   125] loss: 0.796\n",
            "[5,   127] loss: 0.792\n",
            "[5,   129] loss: 0.772\n",
            "[5,   131] loss: 0.788\n",
            "[5,   133] loss: 0.798\n"
          ]
        }
      ],
      "source": [
        "lstmTrainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIoVG-GVQMnB",
        "outputId": "1169508a-f164-4719-cffc-34bacee93877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.671\n"
          ]
        }
      ],
      "source": [
        "lstmTrainer.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fv8EhWuH_BX7"
      },
      "outputs": [],
      "source": [
        "lstmTrainer.calcluate_accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAE0nw0e-yth"
      },
      "outputs": [],
      "source": [
        "lstmTrainer.save_model(2)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
