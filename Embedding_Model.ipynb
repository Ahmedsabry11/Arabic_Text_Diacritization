{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qd5IuOVjtji3"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade pip\n",
        "# !pip install gensim\n",
        "# !pip install nltk\n",
        "# !pip install tokenizers\n",
        "# !pip install sentencepiece\n",
        "# !pip install python-bidi\n",
        "# !pip install arabic-reshaper\n",
        "# # !pip install PyArabic\n",
        "# !pip install spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GVof0I1Bvhfm"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DIACRITIC2INDEX:  dict_items([('ً', 0), ('ٌ', 1), ('ٍ', 2), ('َ', 3), ('ُ', 4), ('ِ', 5), ('ّ', 6), ('ًّ', 7), ('ٌّ', 8), ('ٍّ', 9), ('َّ', 10), ('ُّ', 11), ('ِّ', 12), ('ْ', 13), ('', 14), ('0', 15)])\n",
            "{'ً': 0, 'ٌ': 1, 'ٍ': 2, 'َ': 3, 'ُ': 4, 'ِ': 5, 'ّ': 6, 'ًّ': 7, 'ٌّ': 8, 'ٍّ': 9, 'َّ': 10, 'ُّ': 11, 'ِّ': 12, 'ْ': 13, '': 14, '0': 15}\n",
            "16\n",
            "{'ء': 0, 'آ': 1, 'أ': 2, 'ؤ': 3, 'إ': 4, 'ئ': 5, 'ا': 6, 'ب': 7, 'ة': 8, 'ت': 9, 'ث': 10, 'ج': 11, 'ح': 12, 'خ': 13, 'د': 14, 'ذ': 15, 'ر': 16, 'ز': 17, 'س': 18, 'ش': 19, 'ص': 20, 'ض': 21, 'ط': 22, 'ظ': 23, 'ع': 24, 'غ': 25, 'ف': 26, 'ق': 27, 'ك': 28, 'ل': 29, 'م': 30, 'ن': 31, 'ه': 32, 'و': 33, 'ى': 34, 'ي': 35, ' ': 36, '0': 37, 's': 38}\n",
            "39\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "# from torchvision.transforms import ToTensor\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch.nn.functional as F\n",
        "import pandas as pd\n",
        "# from pyarabic.araby import tokenize, strip_tashkeel\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from utilities import *\n",
        "import textProcessing as tp\n",
        "import nltk\n",
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import word2vec\n",
        "import spacy\n",
        "# nltk.download('punkt')\n",
        "# train_text = load_text(\"dataset/train.txt\")\n",
        "# tp.preprocessing_text(train_text,\"train_preprocessed.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.1.2+cu118\n"
          ]
        }
      ],
      "source": [
        "print(torch.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ilaf9_Jmxsg4",
        "outputId": "af18fa5a-65ca-4e18-e441-498d3c19fd05"
      },
      "outputs": [],
      "source": [
        "# Download via terminal commands\n",
        "# !wget \"https://archive.org/download/aravec2.0/tweet_cbow_300.zip\"\n",
        "# !unzip \"tweet_cbow_300.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48wnuE9TICHz",
        "outputId": "e8dc02b6-de30-4842-a3fe-a342b9057e19"
      },
      "outputs": [],
      "source": [
        "# # # make a directory called \"spacyModel\"\n",
        "# # %mkdir spacyModel\n",
        "# # model = gensim.models.Word2Vec.load(\"tweets_cbow_300\")\n",
        "# # # export the word2vec fomart to the directory\n",
        "# # model.wv.save_word2vec_format(\"./spacyModel/aravec.txt\")\n",
        "# # # using `gzip` to compress the .txt file\n",
        "# # !gzip ./spacyModel/aravec.txt\n",
        "# # !python -m spacy init vectors ar ./spacyModel/aravec.txt.gz spacy.aravec.model\n",
        "nlp = spacy.load(\"./spacy.aravec.model/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6a6o7z8GsZnS"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.models import word2vec\n",
        "from torch import mode\n",
        "\n",
        "class AraVecEmbbedding():\n",
        "    def __init__(self,model):\n",
        "        self.model=model\n",
        "\n",
        "    def map_words_to_vectors(self,sentence):\n",
        "        return [(token.text,token.vector) for token in self.model(sentence)]\n",
        "\n",
        "\n",
        "    def AraVec_wordEmbedding(self,sentence):\n",
        "        return self.map_words_to_vectors(sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "6nUKjuy7shiG"
      },
      "outputs": [],
      "source": [
        "from utilities import *\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import textProcessing as tp\n",
        "import torch\n",
        "import numpy as np\n",
        "from data_preprocessing import DataPreprocessing\n",
        "import re\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self,model,dataset_path=\"dataset/train_preprocessed.txt\",T=100):\n",
        "        self.dataPreprocessor = DataPreprocessing()\n",
        "        self.T = T\n",
        "        self.data = load_text(dataset_path)\n",
        "        self.wordEmbedding=AraVecEmbbedding(model)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        sentence = self.data[idx]\n",
        "        # extract the label\n",
        "        labels,sentence= self.dataPreprocessor.extract_diacritics_with_previous_letter('s'+sentence)\n",
        "\n",
        "        # check sentence length\n",
        "        if len(sentence) > self.T:\n",
        "            sentence = sentence[:self.T]\n",
        "            labels = labels[:self.T]\n",
        "        else:\n",
        "            for i in range(self.T - len(sentence)):\n",
        "                sentence += '0'\n",
        "                labels.append('0')\n",
        "\n",
        "        assert len(sentence) == self.T\n",
        "        assert len(labels) == self.T\n",
        "\n",
        "        # convert the sentence to embedding\n",
        "        # Word\n",
        "        arabic_words_with_spaces =  re.findall(r'\\s*\\b\\w+\\b\\s*|,\\s*|\\s*\\b\\w+\\b(?!\\s*$)',sentence)\n",
        "        sentence_char = self.dataPreprocessor.convert_sentence_to_vector(sentence)\n",
        "        sentence_word_embedding =self.wordEmbedding.AraVec_wordEmbedding(sentence)\n",
        "        list_of_char = []\n",
        "        k=0\n",
        "        for i, (word, word_embedding) in enumerate(sentence_word_embedding):\n",
        "            length_arabic = len(arabic_words_with_spaces[i])\n",
        "            for j in range(k,k+length_arabic):\n",
        "                li = sentence_char[j].astype(float)\n",
        "                concatenated_array = np.concatenate((li, np.array(word_embedding)), axis=0)\n",
        "                list_of_char.append(concatenated_array)\n",
        "            k+=length_arabic\n",
        "        assert len(list_of_char) == self.T\n",
        "        assert len(labels) == self.T\n",
        "        # convert the labels to one hot encoding\n",
        "        labels = self.dataPreprocessor.convert_labels_to_indices(labels)\n",
        "\n",
        "        # reshape the labels\n",
        "        labels = labels.reshape(-1,1)\n",
        "\n",
        "        # convert the sentence and labels to tensors\n",
        "        sentence = torch.tensor(list_of_char, dtype=torch.float32)\n",
        "        labels = torch.LongTensor(labels)\n",
        "\n",
        "        return sentence, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LA2yfGo0Wjem"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from LSTM import LSTMClassifier\n",
        "from data_preprocessing import DataPreprocessing\n",
        "\n",
        "class LSTMTrainer:\n",
        "    def __init__(self,load=True,epoch = 0,input_size = 39,hidden_size = 128,output_size = 16,batch_size = 512,num_epochs = 20):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.batch_size = batch_size\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.model = LSTMClassifier(self.input_size, self.hidden_size, self.output_size)\n",
        "        self.current_epoch = 0\n",
        "        self.current_epoch = epoch\n",
        "        if load:\n",
        "            self.load_model(epoch)\n",
        "        self.model.to(self.device)\n",
        "        self.dataset = MyDataset(model = nlp,T = 280)\n",
        "        self.test_dataset = MyDataset(model = nlp,dataset_path=\"dataset/test_preprocessed.txt\",T = 280)\n",
        "        self.train_dataloader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=False)\n",
        "        self.test_dataloader = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        self.criterion = nn.CrossEntropyLoss(ignore_index=15)\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "        self.scheduler = StepLR(self.optimizer, step_size=2, gamma=0.5)\n",
        "\n",
        "\n",
        "    def train(self):\n",
        "        for epoch in range(0,self.current_epoch):\n",
        "            self.scheduler.step()\n",
        "        for epoch in range(self.current_epoch,self.num_epochs):\n",
        "            self.model.train()\n",
        "            running_loss = 0.0\n",
        "            for i, data in enumerate(self.train_dataloader, 0):\n",
        "                # get the inputs\n",
        "                inputs, labels = data\n",
        "                # print(\"input shape\",inputs.size())\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "                # zero the parameter gradients\n",
        "                self.optimizer.zero_grad()\n",
        "                # forward + backward + optimize\n",
        "                outputs = self.model(inputs)\n",
        "                outputs = outputs.view(-1, outputs.shape[-1])\n",
        "                labels = labels.view(-1)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
        "                self.optimizer.step()\n",
        "                # print statistics\n",
        "                running_loss += loss.item()\n",
        "                if i % 2 == 0:  # print every 100 mini-batches\n",
        "                    print('[%d, %5d] loss: %.3f' %\n",
        "                          (epoch + 1, i + 1, running_loss / 2))\n",
        "                    running_loss = 0.0\n",
        "                if i%500==499:      \n",
        "                # save the model if it has the best training loss till now\n",
        "                    self.save_model(epoch+1)\n",
        "            self.scheduler.step()\n",
        "\n",
        "        print('Finished Training')\n",
        "\n",
        "    def test(self):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            running_loss = 0.0\n",
        "            for i, data in enumerate(self.test_dataloader, 0):\n",
        "                # get the inputs\n",
        "                inputs, labels = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "                # forward + backward + optimize\n",
        "                outputs = self.model(inputs)\n",
        "                outputs = outputs.view(-1, outputs.shape[-1])\n",
        "                labels = labels.view(-1)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "                running_loss += loss.item()\n",
        "            print('Test loss: %.3f' %\n",
        "                  (running_loss / len(self.test_dataloader)))\n",
        "    def calcluate_accuracy(self):\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for i, data in enumerate(self.test_dataloader, 0):\n",
        "                # get the inputs\n",
        "                inputs, labels = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "                # forward + backward + optimize\n",
        "                outputs = self.model(inputs)\n",
        "                outputs = outputs.view(-1, outputs.shape[-1])\n",
        "                labels = labels.view(-1)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                      # cut the padding\n",
        "                predicted = predicted[labels != 15]\n",
        "                labels = labels[labels != 15]\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            print('Accuracy of the network on the test set: %d %%' % (\n",
        "                    100 * correct / total))\n",
        "            print('Accuracy of the network on the test set: %f %%' % (\n",
        "                    100 * correct / total))\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            for i, data in enumerate(self.train_dataloader, 0):\n",
        "                # get the inputs\n",
        "                inputs, labels = data\n",
        "                inputs = inputs.to(self.device)\n",
        "                labels = labels.to(self.device)\n",
        "                # forward + backward + optimize\n",
        "                outputs = self.model(inputs)\n",
        "                outputs = outputs.view(-1, outputs.shape[-1])\n",
        "                labels = labels.view(-1)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                      # cut the padding\n",
        "                predicted = predicted[labels != 15]\n",
        "                labels = labels[labels != 15]\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "            print('Accuracy of the network on the train set: %d %%' % (\n",
        "                    100 * correct / total))\n",
        "            # print floating point accuracy\n",
        "            print('Accuracy of the network on the train set: %f %%' % (\n",
        "                    100 * correct / total))\n",
        "    def save_model(self,epoch):\n",
        "        torch.save(self.model.state_dict(), \"models/lstm_model_word_Embedd\"+str(epoch)+\".pth\")\n",
        "    def load_model(self,epoch=9):\n",
        "        self.model.load_state_dict(torch.load(\"models/lstm_model_\"+str(epoch)+\".pth\"))\n",
        "        self.model.eval()\n",
        "        self.current_epoch = epoch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "34N76AryvKcd"
      },
      "outputs": [],
      "source": [
        "# from LSTM_Training import LSTMTrainer\n",
        "lstmTrainer = LSTMTrainer(load = False,epoch = 0,input_size = 339,hidden_size = 1024,output_size = 16,batch_size = 16,num_epochs = 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import gc\n",
        "# import torch\n",
        "# torch.cuda.empty_cache()\n",
        "# gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LccF7Ac0UC5",
        "outputId": "fac47163-5ea7-44e1-f730-50ee075136e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5,  4235] loss: 0.014\n",
            "[5,  4237] loss: 0.008\n",
            "[5,  4239] loss: 0.010\n",
            "[5,  4241] loss: 0.008\n",
            "[5,  4243] loss: 0.007\n",
            "[5,  4245] loss: 0.009\n",
            "[5,  4247] loss: 0.004\n",
            "[5,  4249] loss: 0.008\n",
            "[5,  4251] loss: 0.008\n",
            "[5,  4253] loss: 0.005\n",
            "[5,  4255] loss: 0.009\n",
            "[5,  4257] loss: 0.006\n",
            "[5,  4259] loss: 0.007\n",
            "[5,  4261] loss: 0.008\n",
            "[5,  4263] loss: 0.004\n",
            "[5,  4265] loss: 0.005\n",
            "[5,  4267] loss: 0.006\n",
            "[5,  4269] loss: 0.014\n",
            "[5,  4271] loss: 0.005\n",
            "[5,  4273] loss: 0.011\n",
            "[5,  4275] loss: 0.008\n",
            "[5,  4277] loss: 0.012\n",
            "[5,  4279] loss: 0.008\n",
            "[5,  4281] loss: 0.008\n",
            "[5,  4283] loss: 0.008\n",
            "[5,  4285] loss: 0.010\n",
            "[5,  4287] loss: 0.007\n",
            "[5,  4289] loss: 0.006\n",
            "[5,  4291] loss: 0.011\n",
            "[5,  4293] loss: 0.007\n",
            "[5,  4295] loss: 0.006\n",
            "[5,  4297] loss: 0.008\n",
            "[5,  4299] loss: 0.009\n",
            "[5,  4301] loss: 0.009\n",
            "[5,  4303] loss: 0.010\n",
            "[5,  4305] loss: 0.007\n",
            "[5,  4307] loss: 0.007\n",
            "[5,  4309] loss: 0.006\n",
            "[5,  4311] loss: 0.005\n",
            "[5,  4313] loss: 0.009\n",
            "[5,  4315] loss: 0.005\n",
            "[5,  4317] loss: 0.007\n",
            "[5,  4319] loss: 0.006\n",
            "[5,  4321] loss: 0.006\n",
            "[5,  4323] loss: 0.005\n",
            "[5,  4325] loss: 0.010\n",
            "[5,  4327] loss: 0.009\n",
            "[5,  4329] loss: 0.008\n",
            "[5,  4331] loss: 0.008\n",
            "[5,  4333] loss: 0.005\n",
            "[5,  4335] loss: 0.006\n",
            "[5,  4337] loss: 0.006\n",
            "[5,  4339] loss: 0.012\n",
            "[5,  4341] loss: 0.008\n",
            "[5,  4343] loss: 0.009\n",
            "[5,  4345] loss: 0.006\n",
            "[5,  4347] loss: 0.006\n",
            "[5,  4349] loss: 0.005\n",
            "[5,  4351] loss: 0.005\n",
            "[5,  4353] loss: 0.005\n",
            "[5,  4355] loss: 0.006\n",
            "[5,  4357] loss: 0.007\n",
            "[5,  4359] loss: 0.006\n",
            "[5,  4361] loss: 0.007\n",
            "[5,  4363] loss: 0.004\n",
            "[5,  4365] loss: 0.010\n",
            "[5,  4367] loss: 0.008\n",
            "[5,  4369] loss: 0.008\n",
            "[5,  4371] loss: 0.010\n",
            "[5,  4373] loss: 0.004\n",
            "[5,  4375] loss: 0.003\n",
            "[5,  4377] loss: 0.009\n",
            "[5,  4379] loss: 0.005\n",
            "[5,  4381] loss: 0.014\n",
            "[5,  4383] loss: 0.010\n",
            "[5,  4385] loss: 0.009\n",
            "[5,  4387] loss: 0.008\n",
            "[5,  4389] loss: 0.006\n",
            "[5,  4391] loss: 0.018\n",
            "[5,  4393] loss: 0.005\n",
            "[5,  4395] loss: 0.012\n",
            "[5,  4397] loss: 0.015\n",
            "[5,  4399] loss: 0.009\n",
            "[5,  4401] loss: 0.006\n",
            "[5,  4403] loss: 0.010\n",
            "[5,  4405] loss: 0.007\n",
            "[5,  4407] loss: 0.007\n",
            "[5,  4409] loss: 0.005\n",
            "[5,  4411] loss: 0.008\n",
            "[5,  4413] loss: 0.006\n",
            "[5,  4415] loss: 0.013\n",
            "[5,  4417] loss: 0.006\n",
            "[5,  4419] loss: 0.010\n",
            "[5,  4421] loss: 0.005\n",
            "[5,  4423] loss: 0.007\n",
            "[5,  4425] loss: 0.007\n",
            "[5,  4427] loss: 0.010\n",
            "[5,  4429] loss: 0.004\n",
            "[5,  4431] loss: 0.007\n",
            "[5,  4433] loss: 0.010\n",
            "[5,  4435] loss: 0.005\n",
            "[5,  4437] loss: 0.011\n",
            "[5,  4439] loss: 0.011\n",
            "[5,  4441] loss: 0.008\n",
            "[5,  4443] loss: 0.008\n",
            "[5,  4445] loss: 0.007\n",
            "[5,  4447] loss: 0.005\n",
            "[5,  4449] loss: 0.006\n",
            "[5,  4451] loss: 0.011\n",
            "[5,  4453] loss: 0.007\n",
            "[5,  4455] loss: 0.007\n",
            "[5,  4457] loss: 0.011\n",
            "[5,  4459] loss: 0.008\n",
            "[5,  4461] loss: 0.007\n",
            "[5,  4463] loss: 0.005\n",
            "[5,  4465] loss: 0.008\n",
            "[5,  4467] loss: 0.009\n",
            "[5,  4469] loss: 0.004\n",
            "[5,  4471] loss: 0.006\n",
            "[5,  4473] loss: 0.007\n",
            "[5,  4475] loss: 0.006\n",
            "[5,  4477] loss: 0.007\n",
            "[5,  4479] loss: 0.010\n",
            "[5,  4481] loss: 0.006\n",
            "[5,  4483] loss: 0.007\n",
            "[5,  4485] loss: 0.015\n",
            "[5,  4487] loss: 0.010\n",
            "[5,  4489] loss: 0.005\n",
            "[5,  4491] loss: 0.009\n",
            "[5,  4493] loss: 0.007\n",
            "[5,  4495] loss: 0.009\n",
            "[5,  4497] loss: 0.008\n",
            "[5,  4499] loss: 0.007\n",
            "[5,  4501] loss: 0.006\n",
            "[5,  4503] loss: 0.008\n",
            "[5,  4505] loss: 0.007\n",
            "[5,  4507] loss: 0.007\n",
            "[5,  4509] loss: 0.005\n",
            "[5,  4511] loss: 0.006\n",
            "[5,  4513] loss: 0.009\n",
            "[5,  4515] loss: 0.009\n",
            "[5,  4517] loss: 0.007\n",
            "[5,  4519] loss: 0.006\n",
            "[5,  4521] loss: 0.009\n",
            "[5,  4523] loss: 0.007\n",
            "[5,  4525] loss: 0.006\n",
            "[5,  4527] loss: 0.008\n",
            "[5,  4529] loss: 0.009\n",
            "[5,  4531] loss: 0.011\n",
            "[5,  4533] loss: 0.008\n",
            "[5,  4535] loss: 0.008\n",
            "[5,  4537] loss: 0.005\n",
            "[5,  4539] loss: 0.007\n",
            "[5,  4541] loss: 0.004\n",
            "[5,  4543] loss: 0.007\n",
            "[5,  4545] loss: 0.008\n",
            "[5,  4547] loss: 0.008\n",
            "[5,  4549] loss: 0.012\n",
            "[5,  4551] loss: 0.007\n",
            "[5,  4553] loss: 0.009\n",
            "[5,  4555] loss: 0.006\n",
            "[5,  4557] loss: 0.007\n",
            "[5,  4559] loss: 0.007\n",
            "[5,  4561] loss: 0.005\n",
            "[5,  4563] loss: 0.007\n",
            "[5,  4565] loss: 0.013\n",
            "[5,  4567] loss: 0.015\n",
            "[5,  4569] loss: 0.009\n",
            "[5,  4571] loss: 0.005\n",
            "[5,  4573] loss: 0.007\n",
            "[5,  4575] loss: 0.009\n",
            "[5,  4577] loss: 0.006\n",
            "[5,  4579] loss: 0.005\n",
            "[5,  4581] loss: 0.010\n",
            "[5,  4583] loss: 0.005\n",
            "[5,  4585] loss: 0.005\n",
            "[5,  4587] loss: 0.007\n",
            "[5,  4589] loss: 0.018\n",
            "[5,  4591] loss: 0.008\n",
            "[5,  4593] loss: 0.004\n",
            "[5,  4595] loss: 0.007\n",
            "[5,  4597] loss: 0.006\n",
            "[5,  4599] loss: 0.011\n",
            "[5,  4601] loss: 0.008\n",
            "[5,  4603] loss: 0.013\n",
            "[5,  4605] loss: 0.014\n",
            "[5,  4607] loss: 0.009\n",
            "Finished Training\n"
          ]
        }
      ],
      "source": [
        "lstmTrainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIoVG-GVQMnB",
        "outputId": "1169508a-f164-4719-cffc-34bacee93877"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.098\n"
          ]
        }
      ],
      "source": [
        "lstmTrainer.test()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "fv8EhWuH_BX7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test set: 97 %\n",
            "Accuracy of the network on the test set: 97.723398 %\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlstmTrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalcluate_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[9], line 115\u001b[0m, in \u001b[0;36mLSTMTrainer.calcluate_accuracy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    114\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 115\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# get the inputs\u001b[39;49;00m\n\u001b[0;32m    117\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
            "Cell \u001b[1;32mIn[8], line 47\u001b[0m, in \u001b[0;36mMyDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(k,k\u001b[38;5;241m+\u001b[39mlength_arabic):\n\u001b[0;32m     46\u001b[0m     li \u001b[38;5;241m=\u001b[39m sentence_char[j]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m---> 47\u001b[0m     concatenated_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((li, np\u001b[38;5;241m.\u001b[39marray(word_embedding)), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     48\u001b[0m     list_of_char\u001b[38;5;241m.\u001b[39mappend(concatenated_array)\n\u001b[0;32m     49\u001b[0m k\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39mlength_arabic\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "lstmTrainer.calcluate_accuracy()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
